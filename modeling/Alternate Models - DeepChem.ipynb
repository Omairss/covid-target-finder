{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow' from '/miniconda/lib/python3.6/site-packages/tensorflow/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, Conv1D, Flatten, MaxPooling1D,\\\n",
    "                        AveragePooling1D, Concatenate, LeakyReLU, Embedding,\\\n",
    "                        GlobalMaxPooling1D,GlobalAveragePooling1D,GaussianNoise,BatchNormalization,Add, ZeroPadding1D\n",
    "\n",
    "\n",
    "importlib.reload(tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with tf.Session() as sess:\n",
    "    devices = sess.list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    NOTE: Taken from sklearn example\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = [0,1]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #ax.figsize((10, 10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugbankReader = pd.read_csv('../data/intermediate/interactions_preprocessed.csv', chunksize = 1000, iterator = True, converters={\"target_gene_encoded_padded\": \\\n",
    "                                                                                     lambda x: np.array(x.strip(\"[]\").replace(\"'\",\"\").split(\", \")).astype(int)+1,\n",
    "                                                                                    \"drug_fingerprint_encoded_padded\": \\\n",
    "                                                                                    lambda x: np.array(x.strip(\"[]\").replace(\"'\",\"\").split(\", \")).astype(int)+1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drugBankProcessor(reader):\n",
    "    train_df = reader.__next__()\n",
    "    #print(df.head())\n",
    "    train_df_y=list(train_df[\"Y\"])\n",
    "    train_df_tge_len=list(train_df['target_gene_encoded_len'])\n",
    "    train_df_dfngprnt_len=list(train_df['drug_fingerprint_encoded_len'])\n",
    "    train_X0=train_df[\"target_gene_encoded_padded\"].tolist()\n",
    "    train_X1=train_df[\"drug_fingerprint_encoded_padded\"].tolist()\n",
    "    train_X0_df=pd.DataFrame(train_X0)\n",
    "    train_X1_df=pd.DataFrame(train_X1)\n",
    "    train_drugid=train_df[\"drug_id\"].tolist()\n",
    "    train_gene_id=train_df[\"gene_id\"].tolist()\n",
    "    train_X0_array=train_X0_df.values\n",
    "    print(train_X0_df.shape)\n",
    "    train_X1_array=train_X1_df.values\n",
    "    del train_df\n",
    "    train_df=pd.concat([train_X0_df,train_X1_df], axis=1)\n",
    "    train_df[\"Y\"]=train_df_y\n",
    "    del train_df_y\n",
    "    train_df['target_gene_encoded_padded']=train_df_tge_len\n",
    "    del train_df_tge_len\n",
    "    train_df['drug_fingerprint_encoded_padded']=train_df_dfngprnt_len\n",
    "    del train_df_dfngprnt_len\n",
    "    train_df=train_df.astype('int16')\n",
    "    train_df['drug_id']=train_drugid\n",
    "    del train_drugid\n",
    "    train_df['gene_id']=train_gene_id\n",
    "    del train_gene_id\n",
    "    del train_X0_array\n",
    "    del train_X1_array\n",
    "    return(train_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 21222)\n"
     ]
    }
   ],
   "source": [
    "drugbank1=drugBankProcessor(drugbankReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9496</th>\n",
       "      <th>9497</th>\n",
       "      <th>9498</th>\n",
       "      <th>9499</th>\n",
       "      <th>9500</th>\n",
       "      <th>Y</th>\n",
       "      <th>target_gene_encoded_padded</th>\n",
       "      <th>drug_fingerprint_encoded_padded</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>gene_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>303</td>\n",
       "      <td>916</td>\n",
       "      <td>DB02704</td>\n",
       "      <td>BE0004010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>957</td>\n",
       "      <td>644</td>\n",
       "      <td>DB06684</td>\n",
       "      <td>BE0001790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "      <td>195</td>\n",
       "      <td>DB03811</td>\n",
       "      <td>BE0001167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>834</td>\n",
       "      <td>207</td>\n",
       "      <td>DB02095</td>\n",
       "      <td>BE0004952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6546</td>\n",
       "      <td>1216</td>\n",
       "      <td>DB03698</td>\n",
       "      <td>BE0002359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30728 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...  9496  9497  9498  9499  9500  Y  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0  0   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0  0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0  0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0  0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0  0   \n",
       "\n",
       "   target_gene_encoded_padded  drug_fingerprint_encoded_padded  drug_id  \\\n",
       "0                         303                              916  DB02704   \n",
       "1                         957                              644  DB06684   \n",
       "2                         600                              195  DB03811   \n",
       "3                         834                              207  DB02095   \n",
       "4                        6546                             1216  DB03698   \n",
       "\n",
       "     gene_id  \n",
       "0  BE0004010  \n",
       "1  BE0001790  \n",
       "2  BE0001167  \n",
       "3  BE0004952  \n",
       "4  BE0002359  \n",
       "\n",
       "[5 rows x 30728 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugbank1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for chunk in tqdm(drugbankReader):\n",
    "    chunk_df=drugBankProcessor(drugbankReader)\n",
    "    print(chunk_df)\n",
    "    drugbank1=pd.concat([drugbank_bank1,chunk_df], axis=0).reset_index(drop=True)\n",
    "    del chunk_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugbank_processed=pd.read_pickle('../data/intermediate/interactions_preprocessed_chunk1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugbank_y=drugbank_processed[\"Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugbank_processed=drugbank_processed.drop([\"Y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugbank_processed.columns=list(range(30727))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef7456ef6504fb9a7895a0c0b9d4ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30719.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "drop_columns=[]\n",
    "for i in tqdm(range(len(columns)-8)):\n",
    "    a=drugbank_processed[i].sum(axis=0)\n",
    "    if a<=200:\n",
    "        drop_columns.append(i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns.append(30725)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns.append(30726)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30717</th>\n",
       "      <th>30718</th>\n",
       "      <th>30719</th>\n",
       "      <th>30720</th>\n",
       "      <th>30721</th>\n",
       "      <th>30722</th>\n",
       "      <th>30723</th>\n",
       "      <th>30724</th>\n",
       "      <th>30725</th>\n",
       "      <th>30726</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>303</td>\n",
       "      <td>916</td>\n",
       "      <td>DB02704</td>\n",
       "      <td>BE0004010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>957</td>\n",
       "      <td>644</td>\n",
       "      <td>DB06684</td>\n",
       "      <td>BE0001790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "      <td>195</td>\n",
       "      <td>DB03811</td>\n",
       "      <td>BE0001167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>834</td>\n",
       "      <td>207</td>\n",
       "      <td>DB02095</td>\n",
       "      <td>BE0004952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6546</td>\n",
       "      <td>1216</td>\n",
       "      <td>DB03698</td>\n",
       "      <td>BE0002359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "1      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "2      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "3      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "4      0      0      0      0      0      0      0      0      0      0  ...   \n",
       "\n",
       "   30717  30718  30719  30720  30721  30722  30723  30724    30725      30726  \n",
       "0      0      0      0      0      0      0    303    916  DB02704  BE0004010  \n",
       "1      0      0      0      0      0      0    957    644  DB06684  BE0001790  \n",
       "2      0      0      0      0      0      0    600    195  DB03811  BE0001167  \n",
       "3      0      0      0      0      0      0    834    207  DB02095  BE0004952  \n",
       "4      0      0      0      0      0      0   6546   1216  DB03698  BE0002359  \n",
       "\n",
       "[5 rows x 30727 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugbank_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X0=drugbank_processed.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X0_array=train_X0.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 16537)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=100, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(train_X0_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformed_train_X0_array=pca.transform(train_X0_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fc_model(input_shape, nb_classes, nb_layers, layer_width, dropout, activation):\n",
    "    model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(1, activation='softmax'),\n",
    "])\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model=create_fc_model(100,2,3,50,.2,'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: nan - acc: 0.1581\n",
      "Epoch 2/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 3/50\n",
      "42000/42000 [==============================] - 1s 16us/step - loss: nan - acc: 0.1581\n",
      "Epoch 4/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 5/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 6/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 7/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 8/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 9/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 10/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 11/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 12/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 13/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 14/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 15/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 16/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 17/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 18/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 19/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 20/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 21/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 22/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 23/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 24/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 25/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 26/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 27/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 28/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 29/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 30/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 31/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 32/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 33/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 34/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 35/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 36/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 37/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 38/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 39/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 40/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 41/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 42/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 43/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 44/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 45/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 46/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 47/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 48/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 49/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n",
      "Epoch 50/50\n",
      "42000/42000 [==============================] - 0s 9us/step - loss: nan - acc: 0.1581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff720248240>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit(\n",
    "  pca_transformed_train_X0_array, # training data\n",
    "  np.array(drugbank_y), # training targets\n",
    "  epochs=50,\n",
    "  batch_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "model = dc.models.SklearnModel(model_instance=LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dataset.X=pca_transformed_train_X0_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dataset.y=drugbank_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dataset.w=drugbank_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.random((4, 4))\n",
    "labels = np.random.random((4,)) \n",
    "from deepchem.data.datasets import NumpyDataset\n",
    "\n",
    "dataset = NumpyDataset(pca_transformed_train_X0_array, np.array(drugbank_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int16)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Reshape, Conv2D, Flatten, Dense, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keras_model = tf.keras.Sequential([\n",
    "    Reshape((16537, 1)),\n",
    "    Conv2D(filters=32, kernel_size=5, activation=tf.nn.relu),\n",
    "    Conv2D(filters=64, kernel_size=5, activation=tf.nn.relu),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation=tf.nn.relu),\n",
    "    Dense(10),\n",
    "    Softmax()\n",
    "])\n",
    "model = dc.models.Model(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-199-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-199-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /miniconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = dc.data.NumpyDataset(mnist.train.images, mnist.train.labels)\n",
    "valid = dc.data.NumpyDataset(mnist.validation.images, mnist.validation.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = tf.keras.Sequential([\n",
    "    Reshape((28, 28, 1)),\n",
    "    Conv2D(filters=32, kernel_size=5, activation=tf.nn.relu),\n",
    "    Conv2D(filters=64, kernel_size=5, activation=tf.nn.relu),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation=tf.nn.relu),\n",
    "    Dense(10),\n",
    "    Softmax()\n",
    "])\n",
    "keras_model.compile('sgd', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dc.models.Model(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Each model is responsible for its own fit_on_batch method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-b8f5c98301fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/deepchem/deepchem/models/models.py\u001b[0m in \u001b[0;36mfit_on_batch\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     raise NotImplementedError(\n\u001b[0;32m---> 69\u001b[0;31m         \"Each model is responsible for its own fit_on_batch method.\")\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Each model is responsible for its own fit_on_batch method."
     ]
    }
   ],
   "source": [
    "model.fit_on_batch(dataset.X,dataset.y,dataset.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int16)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
